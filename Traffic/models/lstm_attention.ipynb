{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e9c081",
   "metadata": {},
   "source": [
    "# Modelo LSTM Complejo aplicando transformaciones sobre las variables cíclicas y capas de atención.\n",
    "#### Este modelo LSTM busca predecir la intensidad y la velocidad media de los vehículos en un punto dada una entrada de datos.\n",
    "\n",
    "El primer paso es importar todas las librerías necesarias para el correcto funcionamiento de este cuaderno de Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadda1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías estándar\n",
    "from math import sqrt  # Función matemática\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np  # Manejo de arrays numéricos\n",
    "import pandas as pd  # Manipulación de estructuras de datos (DataFrames)\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt  # Gráficas y visualización\n",
    "\n",
    "# Preprocesamiento y métricas (Scikit-learn)\n",
    "from sklearn.preprocessing import StandardScaler # Escalado de características\n",
    "from sklearn.preprocessing import MinMaxScaler  # Normalización de datos\n",
    "from sklearn.model_selection import train_test_split # División de conjuntos de datos en entrenamiento y prueba\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # Métricas de evaluación\n",
    "\n",
    "# Modelado (Keras / TensorFlow)\n",
    "import tensorflow as tf  # Backend de Keras, entrenamiento de modelos\n",
    "import tensorflow.keras.backend as K  # Funciones internas del backend, útil para personalización\n",
    "from tensorflow.keras.models import Model  # Modelo funcional, flexible\n",
    "from tensorflow.keras.layers import Layer, Input, LSTM, Dense, Dropout  # Capas desde API funcional\n",
    "from keras.layers import LSTM, Dense, Dropout  # Capas desde Keras \"puro\"\n",
    "from keras.initializers import GlorotUniform  # Inicializador de pesos\n",
    "from keras.regularizers import l2  # Regularización L2 para evitar sobreajuste\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizador Adam\n",
    "\n",
    "# Guardado/carga de modelos y objetos\n",
    "import joblib  # Serialización de modelos/objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269d73f",
   "metadata": {},
   "source": [
    "Cargamos los datos sobre los que se va a entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_objetivo = 3498 # <--- Sustituir este valor según el objetivo\n",
    "df = pd.read_csv(f'../data/{id_objetivo}.csv')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(f\"Datos cargados para el ID objetivo {id_objetivo}:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd61ef",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos.\n",
    "\n",
    "Pasamos la columna `error` a numérica. La columna `tipo_elem` no la modificamos ya que no se va a contar con ella para el entrenamiento, pues su valor es el mismo todo el rato (estos datos corresponden al mismo elemento todo el rapo, por lo que el tipo de elemento será siempre el mismo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a257e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = (df['error'] != 'N').astype(int)\n",
    "\n",
    "# Vemos si se ha aplicado correctamente la transformación\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf061c",
   "metadata": {},
   "source": [
    "El siguiente paso es transformar las variables cíclicas a través de las funciones seno y coseno para capturar la periodicidad de las características temporales. En este caso, se ha calculado el ciclo diario, semanal, mensual y anual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'fecha' a tipo datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Extraer características de la fecha\n",
    "# Hora exacta en decimal\n",
    "df['hora'] = df['fecha'].dt.hour + df['fecha'].dt.minute / 60.0\n",
    "# Día de la semana\n",
    "df['day_of_week'] = df['fecha'].dt.dayofweek  # Lunes=0, Domingo=6\n",
    "# Mes del año\n",
    "df['month'] = df['fecha'].dt.month # 1 a 12\n",
    "# Día del año\n",
    "df['day_of_year'] = df['fecha'].dt.dayofyear \n",
    "\n",
    "\n",
    "# Aplicar las funciones seno y coseno\n",
    "# Ciclo diario\n",
    "df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "# Ciclo semanal\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "# Ciclo mensual\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "# Ciclo anual (día del año)\n",
    "df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "\n",
    "# Verificar que las nuevas columnas se han añadido correctamente\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28094663",
   "metadata": {},
   "source": [
    "El siguiente paso es verificar que no haya ninguna columna con algún tipo de dato que no queramos, por lo que se imprimen los tipos de dato de los valores de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a32115",
   "metadata": {},
   "source": [
    "Ya empezamos a prepararnos para entrenar el modelo. Se eliminan las filas nulas y se filtra el DataFrame para que solo tenga las columnas que se vayan a emplear para entrenar el modelo, poniendo al final las variables que se van a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4138d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df[['ocupacion', 'carga', 'error', 'periodo_integracion', 'hora_sin', 'hora_cos', \n",
    "         'dow_sin', 'dow_cos', 'doy_sin', 'doy_cos', 'month_sin', 'month_cos', 'intensidad', 'vmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df4aa3",
   "metadata": {},
   "source": [
    "Escalamos los datos para que todas las variables tengan la misma importancia y así el modelo pueda aprender de manera más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae3d53",
   "metadata": {},
   "source": [
    "Creamos secuencias de datos, un proceso importante porque el valor futuro depende del orden y los valores pasados, y de esta manera se captura la dinámica temporal del fenómeno. En este paso también se separan las variables objetivo del resto de columnas.\n",
    "\n",
    "Para cada secuencia se calculan 96 pasos, al ser los datos cada 15 minutos, esto implica que cada secuencia subre 1 día completo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length=96):\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data[i-sequence_length:i, :-2])  # input features\n",
    "        y.append(data[i, -2:])  # target: [intensidad, vmed]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 96\n",
    "X, y = create_sequences(scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec894f",
   "metadata": {},
   "source": [
    "Luego se divide el conjunto de datos en dos: el 80 % se destina al entrenamiento mientras que el 20 % a la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4217de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a65008",
   "metadata": {},
   "source": [
    "### Modelo LSTM.\n",
    "\n",
    "Creamos una clase que representa una capa de atención tipo soft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_score_vec = self.add_weight(\n",
    "            name='attention_score_vec',\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=GlorotUniform(),\n",
    "            trainable=True\n",
    "        )\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, hidden_size)\n",
    "        score = K.tanh(K.dot(inputs, self.attention_score_vec))\n",
    "        attention_weights = K.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = K.sum(context_vector, axis=1)\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd57360",
   "metadata": {},
   "source": [
    "Ahora ya se monta la arquitectura del modelo LSTM intengrando la capa de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdff906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_attention_model(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = Attention()(x)\n",
    "    x = Dropout(0.1)(x)              \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(2)(x)  # intensidad y v_med\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    optimizer = Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b593265",
   "metadata": {},
   "source": [
    "Y se entrena el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5be0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_attention_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b07fc",
   "metadata": {},
   "source": [
    "### Predicciones.\n",
    "\n",
    "Una vez entrenado el modelo, se hacen predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05369c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce05f2",
   "metadata": {},
   "source": [
    "Y se deshace el escalado que se hizo al principio sobre las variables predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_scale(scaled_values, original_data, columns_idx):\n",
    "    unscaled = []\n",
    "    for i in range(len(scaled_values)):\n",
    "        row = np.zeros(original_data.shape[1])\n",
    "        row[columns_idx[0]] = scaled_values[i][0]\n",
    "        row[columns_idx[1]] = scaled_values[i][1]\n",
    "        inv = scaler.inverse_transform([row])\n",
    "        unscaled.append([inv[0][columns_idx[0]], inv[0][columns_idx[1]]])\n",
    "    return np.array(unscaled)\n",
    "\n",
    "y_pred_inv = invert_scale(y_pred, scaled, [-2, -1])\n",
    "y_test_inv = invert_scale(y_test, scaled, [-2, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e35f6",
   "metadata": {},
   "source": [
    "### Evaluación del modelo.\n",
    "\n",
    "Para evaluar el modelo calculamos las métricas MSE, MAE, RMSE y R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc36703",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "rmse = sqrt(mse)\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"MSE:  {mse:.2f}\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97fb3e",
   "metadata": {},
   "source": [
    "Otra buena manera de evaluar un modelo es viualizar gráficas.\n",
    "\n",
    "El siguiente fragmento de código hace una comprativa de los valores reales y los predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70048eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_test_inv[:, 0], color='blue', label=\"Valores Reales\")\n",
    "plt.plot(y_pred_inv[:, 0], color='red', linestyle='--', label=f\"Predicciones LSTM - RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Confidence interval (70%)\n",
    "residuals = y_test_inv[:, 0] - y_pred_inv[:, 0]\n",
    "std_res = np.std(residuals)\n",
    "upper = y_pred_inv[:, 0] + std_res\n",
    "lower = y_pred_inv[:, 0] - std_res\n",
    "plt.fill_between(range(len(y_pred_inv)), lower, upper, color='gray', alpha=0.3, label=\"Intervalo de Confianza (70%)\")\n",
    "\n",
    "plt.title(\"Predicciones del Modelo LSTM con Intervalo de Confianza del 70%\")\n",
    "plt.xlabel(\"Índice Temporal (15 min por paso)\")\n",
    "plt.ylabel(\"Intensidad\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504758e7",
   "metadata": {},
   "source": [
    "Se grafican los residuos para ver si los errores están centrados en cero y  de manera simétrica, lo cual indica que el modelo no tiene sesgos sistemáticos y está capturando bien la estructura de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(residuals, color='red', label='LSTM Residuos')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.title(\"Residuos del Modelo LSTM\")\n",
    "plt.xlabel(\"Índice Temporal (15 min por paso)\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(residuals, bins=30, color='salmon', label='LSTM Residuos')\n",
    "plt.title(\"Histograma de Residuos del Modelo LSTM\")\n",
    "plt.xlabel(\"Residuos\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03236a0b",
   "metadata": {},
   "source": [
    "Por último, se dibuja la curva de aprendizaje para ver cómo de bien ha aprendido el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58430514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ca508",
   "metadata": {},
   "source": [
    "### Guardar modelo.\n",
    "\n",
    "El último paso es guardar el modelo entrenado y el escalador para poder usarlos cuando sea necesario, sin tener que ejecutar todo este cuaderno de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42746b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./saved/lstm_attention_model.h5\")\n",
    "joblib.dump(scaler, \"./saved/lstm_attention_scaler.pkl\")\n",
    "\n",
    "print(\"Modelo y scaler guardados correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
