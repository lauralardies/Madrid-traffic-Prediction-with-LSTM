{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9a6004",
   "metadata": {},
   "source": [
    "# Modelo LSTM Complejo aplicando transformaciones sobre las variables c√≠clicas.\n",
    "#### Este modelo LSTM busca predecir la intensidad y la velocidad media de los veh√≠culos en un punto dada una entrada de datos.\n",
    "\n",
    "El primer paso es importar todas las librer√≠as necesarias para el correcto funcionamiento de este cuaderno de Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadda1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar\n",
    "from math import sqrt  # Funci√≥n matem√°tica\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np  # Manejo de arrays num√©ricos\n",
    "import pandas as pd  # Manipulaci√≥n de estructuras de datos (DataFrames)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt  # Gr√°ficas y visualizaci√≥n\n",
    "\n",
    "# Preprocesamiento y m√©tricas (Scikit-learn)\n",
    "from sklearn.preprocessing import MinMaxScaler  # Normalizaci√≥n de datos\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # M√©tricas de evaluaci√≥n\n",
    "\n",
    "# Modelado (Keras / TensorFlow)\n",
    "import tensorflow as tf  # Backend de Keras, entrenamiento de modelos\n",
    "from keras.models import Sequential  # Modelo secuencial\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizador Adam\n",
    "from keras.layers import LSTM, Dense, Dropout  # Capas de red neuronal\n",
    "from keras.regularizers import l2  # Regularizaci√≥n L2\n",
    "\n",
    "# Guardado/carga de modelos y objetos\n",
    "import joblib  # Serializaci√≥n de modelos/objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a09d0",
   "metadata": {},
   "source": [
    "Cargamos los datos sobre los que se va a entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_objetivo = 3498 # <--- Sustituir este valor seg√∫n el objetivo\n",
    "df = pd.read_csv(f'../data/{id_objetivo}.csv')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(f\"Datos cargados para el ID objetivo {id_objetivo}:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ebc43",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos.\n",
    "\n",
    "Pasamos la columna `error` a num√©rica. La columna `tipo_elem` no la modificamos ya que no se va a contar con ella para el entrenamiento, pues su valor es el mismo todo el rato (estos datos corresponden al mismo elemento todo el rapo, por lo que el tipo de elemento ser√° siempre el mismo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206abd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = (df['error'] != 'N').astype(int)\n",
    "\n",
    "# Vemos si se ha aplicado correctamente la transformaci√≥n\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cfcd1e",
   "metadata": {},
   "source": [
    "El siguiente paso es transformar las variables c√≠clicas a trav√©s de las funciones seno y coseno para capturar la periodicidad de las caracter√≠sticas temporales. En este caso, se ha calculado el ciclo diario, semanal, mensual y anual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'fecha' a tipo datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Extraer caracter√≠sticas de la fecha\n",
    "# Hora exacta en decimal\n",
    "df['hora'] = df['fecha'].dt.hour + df['fecha'].dt.minute / 60.0\n",
    "# D√≠a de la semana\n",
    "df['day_of_week'] = df['fecha'].dt.dayofweek  # Lunes=0, Domingo=6\n",
    "# Mes del a√±o\n",
    "df['month'] = df['fecha'].dt.month # 1 a 12\n",
    "# D√≠a del a√±o\n",
    "df['day_of_year'] = df['fecha'].dt.dayofyear \n",
    "\n",
    "\n",
    "# Aplicar las funciones seno y coseno\n",
    "# Ciclo diario\n",
    "df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "# Ciclo semanal\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "# Ciclo mensual\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "# Ciclo anual (d√≠a del a√±o)\n",
    "df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "\n",
    "# Verificar que las nuevas columnas se han a√±adido correctamente\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03caf0c0",
   "metadata": {},
   "source": [
    "El siguiente paso es verificar que no haya ninguna columna con alg√∫n tipo de dato que no queramos, por lo que se imprimen los tipos de dato de los valores de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed419a",
   "metadata": {},
   "source": [
    "Ya empezamos a prepararnos para entrenar el modelo. Se eliminan las filas nulas y se filtra el DataFrame para que solo tenga las columnas que se vayan a emplear para entrenar el modelo, poniendo al final las variables que se van a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4138d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df[['ocupacion', 'carga', 'error', 'periodo_integracion', 'hora_sin', 'hora_cos', \n",
    "         'dow_sin', 'dow_cos', 'doy_sin', 'doy_cos', 'month_sin', 'month_cos', 'intensidad', 'vmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98490fa9",
   "metadata": {},
   "source": [
    "Escalamos los datos para que todas las variables tengan la misma importancia y as√≠ el modelo pueda aprender de manera m√°s eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b99607",
   "metadata": {},
   "source": [
    "Creamos secuencias de datos, un proceso importante porque el valor futuro depende del orden y los valores pasados, y de esta manera se captura la din√°mica temporal del fen√≥meno. En este paso tambi√©n se separan las variables objetivo del resto de columnas.\n",
    "\n",
    "Para cada secuencia se calculan 96 pasos, al ser los datos cada 15 minutos, esto implica que cada secuencia subre 1 d√≠a completo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length=96):\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data[i-sequence_length:i, :-2])  # input features\n",
    "        y.append(data[i, -2:])  # target: [intensidad, vmed]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 96\n",
    "X, y = create_sequences(scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacda475",
   "metadata": {},
   "source": [
    "Luego se divide el conjunto de datos en dos: el 80 % se destina al entrenamiento mientras que el 20 % a la validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76945ad",
   "metadata": {},
   "source": [
    "### Modelo LSTM.\n",
    "\n",
    "Ahora ya se monta la arquitectura del modelo LSTM y se entrena el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Primera capa LSTM\n",
    "model.add(LSTM(128, return_sequences=True, \n",
    "               dropout=0.1, recurrent_dropout=0, \n",
    "               kernel_regularizer=l2(1e-5), \n",
    "               input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "# Segunda capa LSTM\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0, \n",
    "               kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "# Capa densa oculta 1\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(1e-5)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Capa densa oculta 2\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(1e-5)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(Dense(2))  # intensidad y vmed\n",
    "\n",
    "# Compilaci√≥n del modelo\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83eab61",
   "metadata": {},
   "source": [
    "### Predicciones.\n",
    "\n",
    "Una vez entrenado el modelo, se hacen predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05369c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2314ef",
   "metadata": {},
   "source": [
    "Y se deshace el escalado que se hizo al principio sobre las variables predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_scale(scaled_values, original_data, columns_idx):\n",
    "    unscaled = []\n",
    "    for i in range(len(scaled_values)):\n",
    "        row = np.zeros(original_data.shape[1])\n",
    "        row[columns_idx[0]] = scaled_values[i][0]\n",
    "        row[columns_idx[1]] = scaled_values[i][1]\n",
    "        inv = scaler.inverse_transform([row])\n",
    "        unscaled.append([inv[0][columns_idx[0]], inv[0][columns_idx[1]]])\n",
    "    return np.array(unscaled)\n",
    "\n",
    "y_pred_inv = invert_scale(y_pred, scaled, [-2, -1])\n",
    "y_test_inv = invert_scale(y_test, scaled, [-2, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b02189",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n del modelo.\n",
    "\n",
    "Para evaluar el modelo calculamos las m√©tricas MSE, MAE, RMSE y R¬≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc36703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä METRICS\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "rmse = sqrt(mse)\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"MSE:  {mse:.2f}\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R¬≤:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9b757",
   "metadata": {},
   "source": [
    "Otra buena manera de evaluar un modelo es viualizar gr√°ficas.\n",
    "\n",
    "El siguiente fragmento de c√≥digo hace una comprativa de los valores reales y los predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70048eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_test_inv[:, 0], color='blue', label=\"Valores Reales\")\n",
    "plt.plot(y_pred_inv[:, 0], color='red', linestyle='--', label=f\"Predicciones LSTM - RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Confidence interval (70%)\n",
    "residuals = y_test_inv[:, 0] - y_pred_inv[:, 0]\n",
    "std_res = np.std(residuals)\n",
    "upper = y_pred_inv[:, 0] + std_res\n",
    "lower = y_pred_inv[:, 0] - std_res\n",
    "plt.fill_between(range(len(y_pred_inv)), lower, upper, color='gray', alpha=0.3, label=\"Intervalo de Confianza (70%)\")\n",
    "\n",
    "plt.title(\"Predicciones del Modelo LSTM con Intervalo de Confianza del 70%\")\n",
    "plt.xlabel(\"√çndice Temporal (15 min por paso)\")\n",
    "plt.ylabel(\"Intensidad\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42987fd5",
   "metadata": {},
   "source": [
    "Se grafican los residuos para ver si los errores est√°n centrados en cero y  de manera sim√©trica, lo cual indica que el modelo no tiene sesgos sistem√°ticos y est√° capturando bien la estructura de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(residuals, color='red', label='LSTM Residuos')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.title(\"Residuos del Modelo LSTM\")\n",
    "plt.xlabel(\"√çndice Temporal (15 min por paso)\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(residuals, bins=30, color='salmon', label='LSTM Residuos')\n",
    "plt.title(\"Histograma de Residuos del Modelo LSTM\")\n",
    "plt.xlabel(\"Residuos\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd831ee",
   "metadata": {},
   "source": [
    "Por √∫ltimo, se dibuja la curva de aprendizaje para ver c√≥mo de bien ha aprendido el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58430514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbb549",
   "metadata": {},
   "source": [
    "### Guardar modelo.\n",
    "\n",
    "El √∫ltimo paso es guardar el modelo entrenado y el escalador para poder usarlos cuando sea necesario, sin tener que ejecutar todo este cuaderno de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42746b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./saved/lstm_model_cyclical.h5\")\n",
    "joblib.dump(scaler, \"./saved/lstm_scaler_cyclical.pkl\")\n",
    "\n",
    "print(\"Modelo y scaler guardados correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
