{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1caee3a",
   "metadata": {},
   "source": [
    "# Predicciones.\n",
    "#### En este documento se hacen predicciones de la intensidad y velocidad media del mejor modelo entrenado en la carpeta `models` basadas en datos en tiempo real.\n",
    "\n",
    "El primer paso es importar las librerías necesarias para el correcto funcionamiento del cuaderno de Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías estándar\n",
    "import pickle  # Serialización de objetos Python\n",
    "import xml.etree.ElementTree as ET  # Lectura y procesamiento de archivos XML\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np  # Operaciones numéricas con arrays\n",
    "import pandas as pd  # Manipulación de datos tabulares\n",
    "\n",
    "# Modelado y persistencia\n",
    "from keras.models import load_model  # Carga de modelos Keras preentrenados\n",
    "import joblib  # Serialización de modelos y objetos (más eficiente que pickle para modelos grandes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50628f",
   "metadata": {},
   "source": [
    "Ahora ya se inicia la simulación para calcular la distancia y tiempo para cada paso del camino mínimo. Se calculará el tiempo para cada movimiento del trayecto mediante la fórmula t = distancia / velocidad.\n",
    "\n",
    "En este primer caso se realiza la simulación de que se calcula una hora de llegada a partir de una de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4578189",
   "metadata": {},
   "source": [
    "Luego se carga el documento XML en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427dc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('../data/pm.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88faca5a",
   "metadata": {},
   "source": [
    "Ahora, se recopila únicamente la información que nos interesa, que se trata de la información de un subconjunto de todos los datos. Mientras que se recopila, se transforman las columnas para poder meter directamente la información en el modelo y evitar hacer transformaciones de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ef6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_intensidad = {}\n",
    "predicciones_vmed = {}\n",
    "\n",
    "for element in ['6798', '6767', '4486', '4362', '3745', '4376', '4373', '5768', '6800', '3715', '3499', '5775', '9907', '9926', '5412', '3498']:\n",
    "    datos = []\n",
    "    fecha = root.find('.//fecha_hora').text\n",
    "    for pm in root.findall('.//pm'):\n",
    "        idelem = pm.find('idelem')\n",
    "        if idelem.text == element:\n",
    "            ocupacion = float(pm.find('ocupacion').text)\n",
    "            carga = float(pm.find('carga').text)\n",
    "            error = pm.find('error').text\n",
    "\n",
    "            datos.append({\n",
    "                'ocupacion': ocupacion,\n",
    "                'carga': carga,\n",
    "                'error': error,\n",
    "                'periodo_integracion': 15,\n",
    "                'fecha': fecha,\n",
    "                'intensidad': 0,\n",
    "                'vmed': 0\n",
    "            })\n",
    "            \n",
    "    df = pd.DataFrame(datos)\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "    df['hora'] = df['fecha'].dt.hour\n",
    "    df['dow'] = df['fecha'].dt.dayofweek  # Lunes=0\n",
    "    df['doy'] = df['fecha'].dt.dayofyear\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "\n",
    "    df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "    df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "\n",
    "    df['doy_sin'] = np.sin(2 * np.pi * df['doy'] / 365)\n",
    "    df['doy_cos'] = np.cos(2 * np.pi * df['doy'] / 365)\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    df['error'] = (df['error'] != 'N').astype(int)\n",
    "\n",
    "    columnas_modelo = [\n",
    "        'ocupacion', 'carga', 'error', 'periodo_integracion',\n",
    "        'hora_sin', 'hora_cos', 'dow_sin', 'dow_cos',\n",
    "        'doy_sin', 'doy_cos', 'month_sin', 'month_cos', 'intensidad', 'vmed'\n",
    "    ]\n",
    "\n",
    "    X = df[columnas_modelo].values\n",
    "\n",
    "    # Cargar el modelo\n",
    "    modelo = load_model(f\"../models/final/{element}.h5\")\n",
    "\n",
    "    # Cargar el scaler\n",
    "    scaler = joblib.load(f\"../models/final/{element}.pkl\")  # o con pickle.load(open(...))\n",
    "\n",
    "    print('Modelo y scaler cargados correctamente.')\n",
    "\n",
    "    # Escalar como siempre\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # El modelo solo necesita las primeras 12 columnas\n",
    "    X_scaled_model_input = X_scaled[:, :12]\n",
    "\n",
    "    # Reshape para LSTM\n",
    "    X_lstm = X_scaled_model_input.reshape((X_scaled_model_input.shape[0], 1, X_scaled_model_input.shape[1]))\n",
    "\n",
    "    # Predecir\n",
    "    pred = modelo.predict(X_lstm)\n",
    "\n",
    "    # Crear un array lleno de ceros (mismo número de columnas que usó el scaler)\n",
    "    salida_dummy = np.zeros((pred.shape[0], scaler.scale_.shape[0]))\n",
    "\n",
    "    # Insertar tus predicciones en las posiciones correctas (por ejemplo, últimas 2 columnas)\n",
    "    # Suponiendo que intensidad y vmed eran las últimas 2 columnas (índices 12 y 13):\n",
    "    salida_dummy[:, 12:] = pred\n",
    "\n",
    "    # Desescalar todo\n",
    "    salida_original = scaler.inverse_transform(salida_dummy)\n",
    "\n",
    "    # Extraer solo la parte que nos interesa\n",
    "    intensidad_pred, vmed_pred = salida_original[:, 12], salida_original[:, 13]\n",
    "\n",
    "    predicciones_intensidad[element] = intensidad_pred[0]\n",
    "    predicciones_vmed[element] = round(vmed_pred[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fcc88",
   "metadata": {},
   "source": [
    "Las velocidades rpedichas valen la mayoría 0 ya que esta variablestá disponible únicamente para elementos interurbanos, y en este caso la mayoría de los IDS sin urbanos. Para estimar la velocidad, se hará proporcionalmente a las intensidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ab2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregir_velocidades(velocidades, intensidades, min_vel=5, max_vel=30):\n",
    "    '''\n",
    "    Corrige las velocidades nulas asignando proporcionalmente a las intensidades.\n",
    "    '''\n",
    "    # IDs con velocidad 0 o muy baja (por seguridad usamos un umbral pequeño, por si hay floats tipo 0.0)\n",
    "    ids_corregir = [id_ for id_, v in velocidades.items() if np.isclose(v, 0.0)]\n",
    "\n",
    "    # Obtener intensidades correspondientes a esos IDs\n",
    "    intensidades_corregir = {id_: intensidades[id_] for id_ in ids_corregir}\n",
    "\n",
    "    # Extraer valores de intensidad\n",
    "    valores_intensidad = np.array(list(intensidades_corregir.values()), dtype=float)\n",
    "\n",
    "    min_int = valores_intensidad.min()\n",
    "    max_int = valores_intensidad.max()\n",
    "\n",
    "    # Evitar división por cero (todos tienen misma intensidad)\n",
    "    if np.isclose(min_int, max_int):\n",
    "        velocidad_media = (min_vel + max_vel) / 2\n",
    "        for id_ in ids_corregir:\n",
    "            velocidades[id_] = np.float64(velocidad_media)\n",
    "        return velocidades\n",
    "\n",
    "    # Asignar velocidades proporcionalmente e inversamente\n",
    "    for id_, intensidad in intensidades_corregir.items():\n",
    "        intensidad_norm = (intensidad - min_int) / (max_int - min_int)  # escalar entre 0 y 1\n",
    "        velocidad = max_vel - intensidad_norm * (max_vel - min_vel)\n",
    "        velocidades[id_] = np.float64(round(velocidad, 2))\n",
    "\n",
    "    return velocidades\n",
    "\n",
    "predicciones_vmed = corregir_velocidades(predicciones_vmed, predicciones_intensidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464e9b4",
   "metadata": {},
   "source": [
    "### Guardamos las predicciones\n",
    "\n",
    "Se guardan las predicciones para poder usarlas para calcular luego el camino mínimo y el tiempo de desplazamiento. También se guarda la fecha de los datos en tiempo real con el fin de obtener automáticamente la hora desde la que basar las simulaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b45ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicciones_intensidad)\n",
    "\n",
    "with open('./predicciones_intensidad.pkl', 'wb') as f:\n",
    "    pickle.dump(predicciones_intensidad, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicciones_vmed)\n",
    "\n",
    "with open('./predicciones_vmed.pkl', 'wb') as f:\n",
    "    pickle.dump(predicciones_vmed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb391b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la fecha para sumar el tiempo que se realiza en el trayecto del camino mínimo\n",
    "\n",
    "with open('./fecha.pkl', 'wb') as f:\n",
    "    pickle.dump(fecha, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
